{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\\%cd '/content/drive/MyDrive/Job Assignment'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Vl6cHRXXhogt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory to your working directory\n",
        "%cd '/content/drive/MyDrive/Job Assignment'\n",
        "\n",
        "# Import necessary packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "\n",
        "# Read the URL file into the pandas object\n",
        "df = pd.read_excel('Input.xlsx')\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "directory_path = '/content/drive/MyDrive/Job Assignment/TitleText'\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "\n",
        "# Loop through each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    url = row['URL']\n",
        "    url_id = row['URL_ID']\n",
        "\n",
        "    # Make a request to the URL\n",
        "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=header)\n",
        "    except:\n",
        "        print(\"Can't get response of {}\".format(url_id))\n",
        "        continue\n",
        "\n",
        "    # Create a BeautifulSoup object\n",
        "    try:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    except:\n",
        "        print(\"Can't get page of {}\".format(url_id))\n",
        "        continue\n",
        "\n",
        "    # Find title\n",
        "    try:\n",
        "        title = soup.find('h1').get_text()\n",
        "    except:\n",
        "        print(\"Can't get title of {}\".format(url_id))\n",
        "        continue\n",
        "\n",
        "    # Find text\n",
        "    article = \"\"\n",
        "    try:\n",
        "        for p in soup.find_all('p'):\n",
        "            article += p.get_text()\n",
        "    except:\n",
        "        print(\"Can't get text of {}\".format(url_id))\n",
        "        continue\n",
        "\n",
        "    # Write title and text to the file\n",
        "    file_name = f'/content/drive/MyDrive/Job Assignment/TitleText/{url_id}.txt'\n",
        "    with open(file_name, 'w') as file:\n",
        "        file.write(title + '\\n' + article)\n",
        "\n",
        "print(\"Text extraction completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFiH4axlkGy7",
        "outputId": "d01d3738-06f8-44f5-e5e7-c3ff74345003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Job Assignment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can't get title of blackassign0036\n",
            "Can't get title of blackassign0049\n",
            "Text extraction completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define directories\n",
        "text_dir = \"/content/drive/MyDrive/Job Assignment/TitleText\"\n",
        "stopwords_dir = \"/content/drive/MyDrive/Job Assignment/StopWords\"\n",
        "sentiment_dir = \"/content/drive/MyDrive/Job Assignment/MasterDictionary\"\n",
        "\n",
        "# Ensure the directory for saving text files exists\n",
        "if not os.path.exists(text_dir):\n",
        "    os.makedirs(text_dir)\n",
        "\n",
        "# Load input data\n",
        "input_df = pd.read_excel('Input.xlsx')\n",
        "\n",
        "# Define a function to extract article title and text\n",
        "def extract_article(url):\n",
        "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
        "    response = requests.get(url, headers=header)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    title = soup.find('h1').get_text() if soup.find('h1') else \"Title not found\"\n",
        "    article = \" \".join([p.get_text() for p in soup.find_all('p')])\n",
        "    if not article:\n",
        "        article = \"Text not found\"\n",
        "\n",
        "    return title, article\n",
        "\n",
        "# Extract articles and save to text files\n",
        "for index, row in input_df.iterrows():\n",
        "    url = row['URL']\n",
        "    url_id = row['URL_ID']\n",
        "\n",
        "    try:\n",
        "        title, article = extract_article(url)\n",
        "        with open(os.path.join(text_dir, f'{url_id}.txt'), 'w') as file:\n",
        "            file.write(f\"{title}\\n{article}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract {url}: {e}\")\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set()\n",
        "for file in os.listdir(stopwords_dir):\n",
        "    with open(os.path.join(stopwords_dir, file), 'r', encoding='ISO-8859-1') as f:\n",
        "        stop_words.update(f.read().splitlines())\n",
        "\n",
        "# Load sentiment words\n",
        "pos_words = set()\n",
        "neg_words = set()\n",
        "for file in os.listdir(sentiment_dir):\n",
        "    with open(os.path.join(sentiment_dir, file), 'r', encoding='ISO-8859-1') as f:\n",
        "        words = f.read().splitlines()\n",
        "        if 'positive' in file:\n",
        "            pos_words.update(words)\n",
        "        elif 'negative' in file:\n",
        "            neg_words.update(words)\n",
        "\n",
        "# Define a function to analyze text and compute metrics\n",
        "def analyze_text(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "\n",
        "    positive_words = [word for word in filtered_words if word.lower() in pos_words]\n",
        "    negative_words = [word for word in filtered_words if word.lower() in neg_words]\n",
        "\n",
        "    positive_score = len(positive_words)\n",
        "    negative_score = len(negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(filtered_words) + 0.000001)\n",
        "\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    num_sentences = len(sentences)\n",
        "    avg_sentence_length = len(filtered_words) / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "    complex_words = [word for word in filtered_words if len(re.findall(r'[aeiouy]', word.lower())) > 2]\n",
        "    percentage_complex_words = len(complex_words) / len(filtered_words) if len(filtered_words) > 0 else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    word_lengths = [len(word) for word in filtered_words]\n",
        "    avg_word_length = sum(word_lengths) / len(word_lengths) if word_lengths else 0\n",
        "\n",
        "    syllable_count = sum(len(re.findall(r'[aeiouy]', word.lower())) for word in filtered_words)\n",
        "    syllables_per_word = syllable_count / len(filtered_words) if len(filtered_words) > 0 else 0\n",
        "\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "    metrics = {\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "        'FOG INDEX': fog_index,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,\n",
        "        'COMPLEX WORD COUNT': len(complex_words),\n",
        "        'WORD COUNT': len(filtered_words),\n",
        "        'SYLLABLE PER WORD': syllables_per_word,\n",
        "        'PERSONAL PRONOUNS': personal_pronouns,\n",
        "        'AVG WORD LENGTH': avg_word_length\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Analyze each text file and collect results\n",
        "results = []\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    file_path = os.path.join(text_dir, f'{url_id}.txt')\n",
        "    if os.path.exists(file_path):\n",
        "        metrics = analyze_text(file_path)\n",
        "        results.append({**row, **metrics})\n",
        "\n",
        "# Convert results to DataFrame\n",
        "output_df = pd.DataFrame(results)\n",
        "\n",
        "# Save the results to CSV\n",
        "output_df.to_csv('Output_Data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Q4b_ihdAnbBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6edb80e9-e68f-4eb9-d86b-8541ff7197e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpmP-iPwHmjW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}